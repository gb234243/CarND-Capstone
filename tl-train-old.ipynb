{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, os.path, sys, logging\n",
    "from functools import reduce\n",
    "import re, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import scipy.ndimage as ndimage\n",
    "import cv2\n",
    "import yaml\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the training/test set information\n",
    "# (adapted from https://github.com/bosch-ros-pkg/bstld/blob/master/read_label_file.py)\n",
    "def get_all_labels(input_yaml, riib=False):\n",
    "    \"\"\" Gets all labels within label file\n",
    "    Note that RGB images are 1280x720 and RIIB images are 1280x736.\n",
    "    :param input_yaml: Path to yaml file\n",
    "    :param riib: If True, change path to labeled pictures\n",
    "    :return: images: Labels for traffic lights\n",
    "    \"\"\"\n",
    "    if not len(input_yaml):\n",
    "        return []\n",
    "\n",
    "    images = yaml.load(open(input_yaml, 'rb').read())\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        images[i]['path'] = os.path.abspath(os.path.join(os.path.dirname(input_yaml), images[i]['path']))\n",
    "        if riib:\n",
    "            images[i]['path'] = images[i]['path'].replace('.png', '.pgm')\n",
    "            images[i]['path'] = images[i]['path'].replace('rgb/train', 'riib/train')\n",
    "            images[i]['path'] = images[i]['path'].replace('rgb/test', 'riib/test')\n",
    "            for box in images[i]['boxes']:\n",
    "                box['y_max'] = box['y_max'] + 8\n",
    "                box['y_min'] = box['y_min'] + 8\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_path_labels(dataset, paths, labels):\n",
    "    for data in dataset:\n",
    "        paths.append(data['path'])\n",
    "\n",
    "        # Determine label\n",
    "        if len(data['boxes']) == 0:\n",
    "            labels.append('NoLight')\n",
    "        else:\n",
    "            labels.append(data['boxes'][0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split into paths and labels\n",
    "paths = []\n",
    "labels = []\n",
    "\n",
    "## Paths\n",
    "simulator_yaml = \"\"\n",
    "rosbag_yaml = \"\"\n",
    "bosch_yaml = \"\"\n",
    "\n",
    "## SIMULATOR DATASET\n",
    "simulator_dataset = get_all_labels(simulator_yaml)\n",
    "split_path_labels(simulator_dataset, paths, labels)\n",
    "\n",
    "## ROSBAG DATASET\n",
    "rosbag_dataset = get_all_labels(rosbag_yaml)\n",
    "split_path_labels(rosbag_dataset, paths, labels)\n",
    "\n",
    "## BOSCH DATASET\n",
    "## (from https://hci.iwr.uni-heidelberg.de/node/6132)\n",
    "bosch_dataset = get_all_labels(bosch_yaml)\n",
    "\n",
    "# Add data from Bosch dataset\n",
    "for data in bosch_dataset:    \n",
    "    # Determine label\n",
    "    num_boxes = len(data['boxes'])\n",
    "    if num_boxes == 0:\n",
    "        labels.append('NoLight')\n",
    "        paths.append(data['path'])\n",
    "    else:\n",
    "        # Check boxes (only select Red, Green, Yellow, and non-conflicting images)\n",
    "        target_label = \"\"\n",
    "        for box in data['boxes']:\n",
    "            # Simplify labels (e.g. RedLeft -> Red)\n",
    "            label_name = box['label']\n",
    "            if label_name[0] == \"R\":\n",
    "                label_name = \"Red\"\n",
    "            elif label_name[0] == \"G\":\n",
    "                label_name = \"Green\"\n",
    "            elif label_name[0] == \"Y\":\n",
    "                label_name = \"Yellow\"\n",
    "            else:\n",
    "                target_label = \"\"\n",
    "                break\n",
    "                \n",
    "            # Set target label\n",
    "            if not len(target_label):\n",
    "                target_label = label_name\n",
    "            \n",
    "            # Compare against target\n",
    "            if target_label != label_name:\n",
    "                target_label = \"\"\n",
    "                break\n",
    "                \n",
    "        if len(target_label):\n",
    "            labels.append(target_label)\n",
    "            paths.append(data['path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train, validation, and test sets\n",
    "# Split ratio is 7:3:1\n",
    "x_train, x_not_train, y_train, y_not_train = train_test_split(paths, labels, test_size=0.3)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_not_train, y_not_train, test_size=0.33)\n",
    "\n",
    "print(\"Train size: {}, Validation size: {}, Test size: {}\".format( \\\n",
    "    len(x_train), len(x_validation), len(x_test))) \n",
    "print(\"All labels in training set:\", set(y_train))\n",
    "print(\"All labels in validation set:\", set(y_validation))\n",
    "print(\"All labels in test set:\", set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Label counts in training set - Needed for oversampling\n",
    "label_counts = {}\n",
    "label_indices = {}\n",
    "for i, label in enumerate(y_train):\n",
    "    if label not in label_counts:\n",
    "        label_counts[label] = 0\n",
    "        label_indices[label] = []\n",
    "    label_counts[label] += 1\n",
    "    label_indices[label].append(i)\n",
    "print(label_counts)\n",
    "print()\n",
    "print(\"Indices with Yellow label:\", label_indices['Yellow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine label seen the most\n",
    "max_count = 0\n",
    "for label in label_counts:\n",
    "    max_count = max(max_count, label_counts[label])\n",
    "\n",
    "# Oversample\n",
    "oversampled_train_indices = []\n",
    "for label in label_indices:\n",
    "    random.shuffle(label_indices[label])\n",
    "    \n",
    "    new_label_set = []\n",
    "    new_label_set += label_indices[label] * int(max_count / len(label_indices[label]))\n",
    "    new_label_set += label_indices[label][0:max_count % len(label_indices[label])]\n",
    "    \n",
    "    oversampled_train_indices += new_label_set\n",
    "    print(\"Label {} now has {} indices\".format(label, len(new_label_set)))\n",
    "    \n",
    "target_indices = oversampled_train_indices\n",
    "#target_indices = list(range(len(x_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadImage(imgPath):\n",
    "    img = cv2.imread(imgPath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.array(cv2.resize(img, (300, 300))) # resize and convert to np.array\n",
    "    img = img / 128 - 1.0 # normalize the image\n",
    "    return img\n",
    "\n",
    "# One Hot Encoder Mapping\n",
    "label_ohe_map = {\n",
    "    'NoLight': [1, 0, 0, 0],\n",
    "    'Red': [0, 1, 0, 0],\n",
    "    'Yellow': [0, 0, 1, 0], \n",
    "    'Green': [0, 0, 0, 1]\n",
    "}\n",
    "\n",
    "## Create Generator\n",
    "NUM_AUGMENTATIONS = 5\n",
    "def createBatchGenerator(indices, x, y, train=False):\n",
    "    def batchGenerator(batch_size):\n",
    "        for i in range(0, NUM_AUGMENTATIONS):\n",
    "            random.shuffle(indices)\n",
    "            for batch_i in range(0, len(indices), batch_size):\n",
    "                images = []\n",
    "                labels = []\n",
    "\n",
    "                for index in indices[batch_i:batch_i + batch_size]:\n",
    "                    image = loadImage(x[index])\n",
    "                    if i == 0:\n",
    "                        # Regular images\n",
    "                        images.append(image)\n",
    "                    elif i == 1:\n",
    "                        # Augment data (flip images along vertical axis)\n",
    "                        # (adapted from Udacity SDC-ND Behavioral Cloning Project)\n",
    "                        images.append(cv2.flip(image, 1))\n",
    "                    elif i == 2:\n",
    "                        # Augment data (add normally-distributed noise)\n",
    "                        # (adapted from Udacity SDC-ND Traffic Sign Classification Project (Term 1))\n",
    "                        image = image + np.random.normal(loc = 0, scale = 0.4,\n",
    "                                                         size = (image.shape[0], image.shape[1], image.shape[2]))\n",
    "                        images.append(image)\n",
    "                    elif i == 3:\n",
    "                        # Augment data (rotate: +15Â°)\n",
    "                        # (adapted from Udacity SDC-ND Traffic Sign Classification Project (Term 1), https://stackoverflow.com/a/19167666)\n",
    "                        images.append(ndimage.rotate(image, 15, reshape=False))\n",
    "                    elif i == 4:\n",
    "                        # Augment data (rotate: -15Â°)\n",
    "                        # (adapted from Udacity SDC-ND Traffic Sign Classification Project (Term 1), https://stackoverflow.com/a/19167666)\n",
    "                        images.append(ndimage.rotate(image, -15, reshape=False))\n",
    "                        \n",
    "                    labels.append(label_ohe_map[y[index]])\n",
    "\n",
    "                yield np.array(images), np.array(labels)\n",
    "\n",
    "            # Training?\n",
    "            if not train:\n",
    "                break;\n",
    "        \n",
    "    return batchGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingBatchGenerator = createBatchGenerator(target_indices, x_train, y_train, train=True)\n",
    "validationBatchGenerator = createBatchGenerator(list(range(len(x_validation))), x_validation, y_validation)\n",
    "testBatchGenerator = createBatchGenerator(list(range(len(x_test))), x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layers\n",
    "def addConv(input, filters, kernel, stride, padding, keepprob):\n",
    "    conv = tf.layers.conv2d(input, filters, kernel, stride, padding=padding, \\\n",
    "                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \\\n",
    "                            activation=tf.nn.relu)\n",
    "    batch_norm = tf.layers.batch_normalization(conv)\n",
    "    dropout = tf.layers.dropout(batch_norm, keepprob)\n",
    "    \n",
    "    return dropout\n",
    "\n",
    "# Networks\n",
    "def createEnsembleCNNUnit(input):\n",
    "    conv1 = addConv(input, 32, 3, 2, 'valid', keepprob)\n",
    "    max1 = tf.layers.max_pooling2d(conv1, 2, 2, padding='valid')\n",
    "\n",
    "    conv2 = addConv(max1, 64, 3, 2, 'valid', keepprob)\n",
    "    max2 = tf.layers.max_pooling2d(conv2, 2, 2, padding='valid')\n",
    "\n",
    "    conv3 = addConv(max2, 128, 3, 2, 'valid', keepprob)\n",
    "    max3 = tf.layers.max_pooling2d(conv3, 2, 2, padding='valid')\n",
    "\n",
    "    flat = tf.contrib.layers.flatten(max3)\n",
    "    fc1 = tf.layers.dense(flat, 128, activation=tf.nn.relu)\n",
    "    fc2 = tf.layers.dense(fc1, 64, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(fc2, 4, activation=tf.nn.relu)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def createEnsembleCnn(input):\n",
    "    unit_1 = createEnsembleCNNUnit(input)\n",
    "    unit_2 = createEnsembleCNNUnit(input)\n",
    "    unit_3 = createEnsembleCNNUnit(input)\n",
    "    logits = tf.add(tf.add(unit_1, unit_2), unit_3, name=\"logits\")\n",
    "\n",
    "    return logits\n",
    "\n",
    "def createSimpleCnn(input):\n",
    "    conv1 = tf.layers.conv2d(input, 8, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max1 = tf.layers.max_pooling2d(conv1, 2, 2, padding='valid')\n",
    "\n",
    "    conv2 = tf.layers.conv2d(max1, 16, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max2 = tf.layers.max_pooling2d(conv2, 2, 2, padding='valid')\n",
    "\n",
    "    conv3 = tf.layers.conv2d(max2, 24, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max3 = tf.layers.max_pooling2d(conv3, 2, 2, padding='valid')\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(max3, 32, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max4 = tf.layers.max_pooling2d(conv4, 2, 2, padding='valid')\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(max4, 40, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max5 = tf.layers.max_pooling2d(conv5, 2, 2, padding='valid')\n",
    "\n",
    "    flat = tf.contrib.layers.flatten(max5)\n",
    "    fc1 = tf.layers.dense(flat, 1024, activation=tf.nn.relu)\n",
    "    fc1_dropout = tf.layers.dropout(fc1, keepprob)\n",
    "    \n",
    "    fc2 = tf.layers.dense(fc1_dropout, 256, activation=tf.nn.relu)\n",
    "    fc2_dropout = tf.layers.dropout(fc2, keepprob)\n",
    "    \n",
    "    fc3 = tf.layers.dense(fc2_dropout, 64, activation=tf.nn.relu)\n",
    "    fc3_dropout = tf.layers.dropout(fc3, keepprob)\n",
    "\n",
    "    logits = tf.layers.dense(fc3_dropout, 4)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def createSimpleCnnSmall(input):\n",
    "    conv1 = tf.layers.conv2d(input, 6, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max1 = tf.layers.max_pooling2d(conv1, 2, 2, padding='valid')\n",
    "\n",
    "    conv2 = tf.layers.conv2d(max1, 8, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max2 = tf.layers.max_pooling2d(conv2, 2, 2, padding='valid')\n",
    "\n",
    "    conv3 = tf.layers.conv2d(max2, 10, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max3 = tf.layers.max_pooling2d(conv3, 2, 2, padding='valid')\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(max3, 12, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max4 = tf.layers.max_pooling2d(conv4, 2, 2, padding='valid')\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(max4, 14, 5, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max5 = tf.layers.max_pooling2d(conv5, 2, 2, padding='valid')\n",
    "\n",
    "    flat = tf.contrib.layers.flatten(max5)\n",
    "    fc1 = tf.layers.dense(flat, 48, activation=tf.nn.relu)\n",
    "    fc1_dropout = tf.layers.dropout(fc1, keepprob)\n",
    "    \n",
    "    fc2 = tf.layers.dense(fc1_dropout, 16, activation=tf.nn.relu)\n",
    "    fc2_dropout = tf.layers.dropout(fc2, keepprob)\n",
    "\n",
    "    logits = tf.layers.dense(fc2_dropout, 4)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def createSimpleCnn3(input):\n",
    "    conv1 = tf.layers.conv2d(input, 6, 3, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max1 = tf.layers.max_pooling2d(conv1, 2, 2, padding='valid')\n",
    "\n",
    "    conv2 = tf.layers.conv2d(max1, 8, 3, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max2 = tf.layers.max_pooling2d(conv2, 2, 2, padding='valid')\n",
    "\n",
    "    conv3 = tf.layers.conv2d(max2, 10, 3, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max3 = tf.layers.max_pooling2d(conv3, 2, 2, padding='valid')\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(max3, 12, 3, 1, padding='valid', activation=tf.nn.relu)\n",
    "    max4 = tf.layers.max_pooling2d(conv4, 2, 2, padding='valid')\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(max4, 14, 3, 1, padding='valid', activation=tf.nn.relu)\n",
    "    batch_norm5 = tf.layers.batch_normalization(conv5)\n",
    "    max5 = tf.layers.max_pooling2d(batch_norm5, 2, 2, padding='valid')\n",
    "\n",
    "    conv6 = tf.layers.conv2d(max5, 16, 3, 1, padding='valid', activation=tf.nn.relu)\n",
    "    batch_norm6 = tf.layers.batch_normalization(conv6)\n",
    "    max6 = tf.layers.max_pooling2d(batch_norm6, 2, 2, padding='valid')\n",
    "        \n",
    "    flat = tf.contrib.layers.flatten(max6)\n",
    "    fc1 = tf.layers.dense(flat, 64, activation=tf.nn.relu)\n",
    "    fc1_dropout = tf.layers.dropout(fc1, keepprob)\n",
    "    \n",
    "    fc2 = tf.layers.dense(fc1_dropout, 16, activation=tf.nn.relu)\n",
    "    fc2_dropout = tf.layers.dropout(fc2, keepprob)\n",
    "\n",
    "    logits = tf.layers.dense(fc2_dropout, 4)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear old graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Input Placeholders\n",
    "images = tf.placeholder(tf.float32, (None, 300, 300, 3), name='input_images')\n",
    "labels = tf.placeholder(tf.float32, (None, 4), name='labels')\n",
    "keepprob = tf.placeholder(tf.float32, name='keep_probability')\n",
    "learningrate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "# Init network\n",
    "#logits = createEnsembleCnn(images)\n",
    "#logits = createSimpleCnn(images)\n",
    "#logits = createSimpleCnnSmall(images)\n",
    "logits = createSimpleCnn3(images)\n",
    "output = tf.nn.softmax(logits, name=\"output\")\n",
    "\n",
    "# Create Optimizer\n",
    "softmax_losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "loss = tf.reduce_mean(softmax_losses)\n",
    "optimizer = tf.train.AdamOptimizer(learningrate).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(labels, 1), tf.argmax(output, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "lr = 5e-4\n",
    "kp = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Network\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "print(\"Training...\")\n",
    "for e in range(epochs):\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Train Network\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for train_images, train_labels in trainingBatchGenerator(batch_size):\n",
    "        _, loss_val, acc = sess.run([optimizer, loss, accuracy], {images: train_images, labels: train_labels, \\\n",
    "                                                                  keepprob: kp, learningrate: lr})\n",
    "        total_loss += loss_val\n",
    "        total_accuracy += acc * train_images.shape[0]\n",
    "\n",
    "    # Determine Validation Accuracy\n",
    "    validation_loss = 0\n",
    "    validation_acc = 0\n",
    "    for valid_images, valid_labels in validationBatchGenerator(batch_size):\n",
    "        loss_val, acc = sess.run([loss, accuracy], {images: valid_images, labels: valid_labels, keepprob: 1.0})\n",
    "        validation_loss += loss_val\n",
    "        validation_acc += acc * valid_images.shape[0]\n",
    "    t_end = time.time()\n",
    "\n",
    "    print(\"Epoch {}/{}, Time {:.2f} s, TrainAccuracy: {:.2f} %, ValAccuracy: {:.2f} %, TrainLoss: {:.2f}, ValLoss: {:.2f}\"\\\n",
    "      .format(e+1, epochs, t_end - t_start, total_accuracy * 100 / len(target_indices) / NUM_AUGMENTATIONS, \\\n",
    "              validation_acc * 100 / len(x_validation), total_loss / NUM_AUGMENTATIONS, validation_loss))\n",
    "    \n",
    "    # Save Network\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, './checkpoints/tl-classifier-%i.ckpt' % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, './checkpoints/tl-classifier-1.ckpt')\n",
    "\n",
    "# Print network\n",
    "for n in tf.get_default_graph().as_graph_def().node:\n",
    "    print(\"%s\" % n.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Test Accuracy\n",
    "total_loss = 0\n",
    "total_acc = 0\n",
    "for test_images, test_labels in testBatchGenerator(batch_size):\n",
    "    loss_val, acc = sess.run([loss, accuracy], {images: test_images, labels: test_labels, keepprob: 1.0})\n",
    "    total_loss += loss_val\n",
    "    total_acc += acc * test_images.shape[0]\n",
    "\n",
    "print(\"Test Loss: {:.5f}, Accuracy: {:.2f}%\".format(total_loss, total_acc * 100 / len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze/Optimize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from https://stackoverflow.com/a/45493657\n",
    "\n",
    "# Save GraphDef\n",
    "tf.train.write_graph(sess.graph_def,'.','model/tl-classifier.pb')\n",
    "# Save checkpoint\n",
    "saver.save(sess=sess, save_path=\"model/tl-classifier\")\n",
    "\n",
    "# Freeze graph\n",
    "from tensorflow.python.tools.freeze_graph import freeze_graph\n",
    "freeze_graph(\"model/tl-classifier.pb\", \"\", False, \"model/tl-classifier\", \"output\",\n",
    "    \"save/restore_all\", \"save/Const:0\", \"model/tl-classifier-frozen.pb\", True, \"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimize for inference\n",
    "# (adapted from tensorflow/python/tools/optimize_for_inference.py, https://stackoverflow.com/a/45493657)\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "# Read frozen model\n",
    "input_graph_def = graph_pb2.GraphDef()\n",
    "with gfile.Open(\"model/tl-classifier-frozen.pb\", \"rb\") as f:\n",
    "    data = f.read()\n",
    "    input_graph_def.ParseFromString(data)\n",
    "\n",
    "# Optimize graph for inference\n",
    "output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "    input_graph_def, [\"input_images\"], [\"output\"], dtypes.float32.as_datatype_enum)\n",
    "\n",
    "# Export optimized graph to file\n",
    "with gfile.FastGFile(\"model/tl-classifier-frozen-opt.pb\", \"w\") as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "# Print optimized graph\n",
    "for op in output_graph_def.node:\n",
    "    print(\"%s\" % op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visually Confirm Working Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_reversed = ['NoLight', 'Red', 'Yellow', 'Green']\n",
    "\n",
    "# Set image size\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "# Load frozen graph (adapted from https://stackoverflow.com/a/45493657)\n",
    "with tf.gfile.GFile('model/tl-classifier-frozen-opt.pb', 'rb') as f:\n",
    "    graph_def_optimized = tf.GraphDef()\n",
    "    graph_def_optimized.ParseFromString(f.read())\n",
    "\n",
    "# Create session (adapted from https://stackoverflow.com/a/45493657)\n",
    "G = tf.Graph()\n",
    "with tf.Session(graph=G) as sess:\n",
    "    image_tensor, output = tf.import_graph_def(graph_def_optimized, return_elements=['input_images:0', 'output:0'])\n",
    "    \n",
    "    # Init global variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # Show misclassified images\n",
    "    print(\"Number of tests: %i\" % len(x_test))\n",
    "    for i, (img_path, label) in enumerate(zip(x_test, y_test)):\n",
    "        # Load Image\n",
    "        test_img = loadImage(img_path)\n",
    "\n",
    "        # Run Classification\n",
    "        prob = sess.run(output, feed_dict={image_tensor: [test_img]})[0]\n",
    "        idx = np.argmax(prob)\n",
    "\n",
    "        # Check\n",
    "        if labels_reversed[idx] != label:\n",
    "            # Print Information\n",
    "            print(\"Mismatch (%i): %s != %s %s\" % (i, labels_reversed[idx], label, np.array_str(prob)))\n",
    "\n",
    "            # Print image\n",
    "            plt.imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
